{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete useless jpgs found in the images folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "u_files = [useless_file \n",
    "           for useless_file in os.listdir('/path/to/images/folder')\n",
    "           if '._' in useless_file]\n",
    "\n",
    "for u_file in u_files:\n",
    "    os.remove(os.path.join('/path/to/images/folder', u_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the folders tree divided by training | validation | test and the resepctive supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230ce8a-ff28-41a7-b5c6-932b86b1a62c.jpg\n",
      "28212471-a0d5-483e-80a9-b4ad009bf25b.jpg\n",
      "e7573626-3a12-4acd-b7eb-1db4a6d9aa42.jpg\n",
      "86d75c9b-af1c-4e52-86fe-9ee9f7987ae8.jpg\n",
      "f91fbc09-e595-48fe-b8af-e3636a52acfd.jpg\n",
      "Dataset organized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "data_path = \"C:/Users/leoac/Downloads/archive/images/images\"\n",
    "processed_prado = \"C:/Users/leoac/OneDrive - Università degli Studi di Milano/Unimi/Subjects/2nd_year/02_quarter/Algorithms/project/processed_prado.feather\"\n",
    "base_dir = \"C:/Users/leoac/Downloads/archive/dataset_3_classes\"\n",
    "\n",
    "# Load the .feather file\n",
    "df = pd.read_feather(processed_prado)\n",
    "\n",
    "# Define train and validation directories\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "# Create directories for each support type\n",
    "support_types = df['support'].unique()\n",
    "for support in support_types:\n",
    "    os.makedirs(os.path.join(train_dir, support), exist_ok=True)\n",
    "    os.makedirs(os.path.join(validation_dir, support), exist_ok=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['support'], random_state=42)\n",
    "\n",
    "# Function to move files to the respective directories\n",
    "def move_files(file_names, src_dir, dest_dir, support_type):\n",
    "    for file_name in file_names:\n",
    "        src_path = os.path.join(src_dir, file_name)\n",
    "        dest_path = os.path.join(dest_dir, support_type, file_name)\n",
    "        try:\n",
    "            shutil.copy(src_path, dest_path)\n",
    "        except:\n",
    "            print(file_name)\n",
    "\n",
    "# Move training files\n",
    "for support in support_types:\n",
    "    train_files = train_df[train_df['support'] == support]['image_file_name'].tolist()\n",
    "    move_files(train_files, data_path, train_dir, support)\n",
    "\n",
    "# Move validation files\n",
    "for support in support_types:\n",
    "    val_files = val_df[val_df['support'] == support]['image_file_name'].tolist()\n",
    "    move_files(val_files, data_path, validation_dir, support)\n",
    "\n",
    "print(\"Dataset organized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 006f3780-ec36-41be-9273-27827d94ece8.jpg from Papel\n",
      "Moved 01cfe574-afd5-499a-b8e4-481b9a2e21a9.jpg from Papel\n",
      "Moved 04edc638-f488-4186-9c79-553025e823df.jpg from Papel\n",
      "Moved 056c975b-9b22-418e-8055-68c5782dc5f3.jpg from Papel\n",
      "Moved 05a68c55-cdba-480f-9edb-4aa2b9d965e7.jpg from Papel\n",
      "Moved 06916e41-571a-4e14-a9a1-6cb7131efdc6.jpg from Papel\n",
      "Moved 0b938333-1ab7-46e3-89c9-156b34530d4a.jpg from Papel\n",
      "Moved 0d88f80a-6be3-43eb-95bb-5e5d2858a1e8.jpg from Papel\n",
      "Moved 0e8a1808-e386-4b5f-b869-3ae5209b0e59.jpg from Papel\n",
      "Moved 0f24cf11-4371-4e32-9f3e-df50ffd682b6.jpg from Papel\n",
      "Moved 03a76a95-80e6-4f28-bdf6-5d4a5136c60b.jpg from Tabla\n",
      "Moved 0561a40c-cca1-4357-a01f-b62801f1183a.jpg from Tabla\n",
      "Moved 05d0ac32-067d-4234-9ff2-3c3f4e2c1b01.jpg from Tabla\n",
      "Moved 08277e8d-bfd2-474e-a4eb-8c443948c74e.jpg from Tabla\n",
      "Moved 0863779a-cff6-4780-a3cd-ce182188ad4f.jpg from Tabla\n",
      "Moved 0b3aee1a-08c6-492a-ad4e-1ab327a5d933.jpg from Tabla\n",
      "Moved 0d3e28cc-c62c-4231-aa43-b3a50de349e4.jpg from Tabla\n",
      "Moved 0d6d199d-c243-4b8b-8fff-456bfc1974f0.jpg from Tabla\n",
      "Moved 0eaecc7e-818c-49ca-8038-b89071466c08.jpg from Tabla\n",
      "Moved 119dc5e1-466c-453e-a458-7169d697eb04.jpg from Tabla\n",
      "Moved 00c8739f-cc77-44f2-8739-fa3c98ef6120.jpg from Lienzo\n",
      "Moved 050d4150-8d36-4535-9bf5-05f1dbbfbafb.jpg from Lienzo\n",
      "Moved 07cfd55e-fcab-45f7-8412-c1be00e895bc.jpg from Lienzo\n",
      "Moved 086534aa-6454-4756-9218-07f6a1393fc3.jpg from Lienzo\n",
      "Moved 094fdc81-1ec1-4cc2-a1bb-051aa5e0af1b.jpg from Lienzo\n",
      "Moved 0a5dea84-1563-48f5-ba2e-5eb3c7f4e18e.jpg from Lienzo\n",
      "Moved 0b490990-3666-4ac5-871d-cb36f0a551c5.jpg from Lienzo\n",
      "Moved 0b60050b-0fbf-476b-9e31-56f6710f6cde.jpg from Lienzo\n",
      "Moved 0b6b30b9-8792-4f0a-bbd7-7cfa5a547452.jpg from Lienzo\n",
      "Moved 0b90edde-f6a7-44d5-85f7-4ffe7fe26fd3.jpg from Lienzo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset_dir = r'C:\\Users\\leoac\\Downloads\\archive\\dataset_3_classes'\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# Create the test folder\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "# Function to move images from a specific category\n",
    "def move_to_test(support):\n",
    "    src_subdir = os.path.join(dataset_dir, 'validation', support)\n",
    "    dst_subdir = os.path.join(test_dir, support)\n",
    "    \n",
    "    # Ensure the destination subdir exists\n",
    "    if not os.path.exists(dst_subdir):\n",
    "        os.makedirs(dst_subdir)\n",
    "    \n",
    "    # Move 10 images from the source subdir to the destination subdir\n",
    "    for filename in os.listdir(src_subdir)[:10]:\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):  # Adjust the extensions as needed\n",
    "            src_file = os.path.join(src_subdir, filename)\n",
    "            dst_file = os.path.join(dst_subdir, filename)\n",
    "            shutil.move(src_file, dst_file)\n",
    "            print(f'Moved {filename} from {support}')\n",
    "\n",
    "# Call the function for each support\n",
    "support_types = ['Papel', 'Tabla', 'Lienzo']\n",
    "for support in support_types:\n",
    "    move_to_test(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the rows with missing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_drop = [\n",
    "    '4230ce8a-ff28-41a7-b5c6-932b86b1a62c.jpg',\n",
    "    '28212471-a0d5-483e-80a9-b4ad009bf25b.jpg',\n",
    "    'e7573626-3a12-4acd-b7eb-1db4a6d9aa42.jpg',\n",
    "    '86d75c9b-af1c-4e52-86fe-9ee9f7987ae8.jpg',\n",
    "    'f91fbc09-e595-48fe-b8af-e3636a52acfd.jpg'\n",
    "    ]\n",
    "df = df.loc[~df['image_file_name'].isin(values_to_drop)]\n",
    "df = df.reset_index(drop=True)\n",
    "df.to_feather('processed_prado.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load on GCP bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from google.cloud import storage\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(bucket_name, local_path, remote_path):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(remote_path)\n",
    "    blob.upload_from_filename(local_path)\n",
    "    print(f'File {local_path} uploaded to {remote_path}.')\n",
    "\n",
    "def upload_files_to_gcs(bucket_name, source_folder):\n",
    "    files_to_upload = []\n",
    "\n",
    "    # Walk through the directory to collect files\n",
    "    for root, dirs, files in tqdm(os.walk(source_folder)):\n",
    "        for file in tqdm(files):\n",
    "            local_path = os.path.join(root, file)\n",
    "            remote_path = os.path.relpath(local_path, source_folder)\n",
    "            files_to_upload.append((bucket_name, local_path, remote_path))\n",
    "\n",
    "    # Use ThreadPoolExecutor to upload files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        futures = [executor.submit(upload_file, *args) for args in files_to_upload]\n",
    "        for future in tqdm(as_completed(futures)):\n",
    "            future.result()  # This will raise an exception if the upload failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 9939.11it/s]\n",
      "\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "100%|██████████| 585/585 [00:00<00:00, 40552.47it/s]\n",
      "\n",
      "100%|██████████| 586/586 [00:00<00:00, 24431.79it/s]\n",
      "\n",
      "100%|██████████| 583/583 [00:00<00:00, 24756.05it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "100%|██████████| 136/136 [00:00<00:00, 24851.89it/s]\n",
      "\n",
      "100%|██████████| 137/137 [00:00<00:00, 13661.58it/s]\n",
      "\n",
      "100%|██████████| 137/137 [00:00<00:00, 21389.95it/s]\n",
      "13it [00:00, 77.82it/s]\n",
      "0it [00:04, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'prado-images-bucket'\n",
    "source_folder = r\"C:\\Users\\leoac\\Downloads\\archive\\dataset_3_classes\"\n",
    "# for root, dirs, files in os.walk(source_folder):\n",
    "#     print(root, dirs, files)\n",
    "#     for file in files:\n",
    "#         local_path = os.path.join(root, file)\n",
    "#         print(os.path.relpath(local_path, source_folder))\n",
    "#         break\n",
    "upload_files_to_gcs(bucket_name, source_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
